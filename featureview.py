# -*- coding: utf-8 -*-
"""FeatureView.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11NSbKvNmjdoTY66b1tkXyRFIix9qCICl
"""

PATH = "/content/cleaned_feature_view.csv"
ID_COL = "consumer_id"
DATE_COL = "score_date"

"""EXPLORATORY DATA ANALYSIS (EDA)"""

import re, numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns
from numpy.linalg import lstsq

def clean_cols(df):
    df = df.copy()
    df.columns = (
        df.columns
          .str.strip()
          .str.replace(r"\s+", "_", regex=True)
          .str.lower()
          .str.replace(r"[^0-9a-zA-Z_]", "", regex=True)
    )
    return df

def iqr_bounds(s,k=1.5):
    q1,q3=s.quantile(.25),s.quantile(.75); i=q3-q1; return q1-k*i,q3+k*i

def compute_vif(x):
    if x.shape[1]<2: return pd.DataFrame(columns=["feature","vif","aux_r2"])
    X=x.copy().fillna(x.median(numeric_only=True))
    X=(X-X.mean())/X.std(ddof=0); X=X.replace([np.inf,-np.inf],0).fillna(0)
    cols=X.columns.tolist(); out=[]
    for col in cols:
        y=X[col].values; Z=X.drop(columns=[col]).values
        Z=np.column_stack([np.ones(Z.shape[0]),Z]); b,*_=lstsq(Z,y,rcond=None)
        yhat=Z@b; ssr=((y-yhat)**2).sum(); sst=((y-y.mean())**2).sum()
        r2=1-ssr/sst if sst>0 else 0.0; vif=1/(1-r2) if (1-r2)>1e-12 else np.inf
        out.append((col,float(vif),float(r2)))
    return pd.DataFrame(out,columns=["feature","vif","aux_r2"]).sort_values("vif",ascending=False)

cols0 = pd.read_csv(PATH, nrows=0).columns
parse_dates = [c for c in ["score_date"] if c in cols0]
df = pd.read_csv(PATH, parse_dates=parse_dates)
df = clean_cols(df)

print("shape:", df.shape)
if ID_COL in df.columns: print("id_unique:", df[ID_COL].is_unique)
print("duplicates_total:", int(df.duplicated().sum()))

dtypes_df = df.dtypes.rename("dtype").to_frame().reset_index().rename(columns={"index":"column"})
display(dtypes_df)

missing_df = df.isna().mean().sort_values(ascending=False).rename("missing_rate").to_frame().reset_index().rename(columns={"index":"column"})
display(missing_df.head(30))

num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
print("numeric_columns:", len(num_cols))

rows=[]
for c in num_cols:
    s=df[c]
    if s.notna().sum()<10: continue
    lo,up=iqr_bounds(s); rate=((s<lo)|(s>up)).mean()
    rows.append((c,float(rate),float(lo),float(up)))
iqr_df=pd.DataFrame(rows,columns=["feature","outlier_rate_iqr","iqr_lower","iqr_upper"]).sort_values("outlier_rate_iqr",ascending=False)
display(iqr_df.head(20))

top_feats = [c for c in iqr_df["feature"].head(12) if c in num_cols]
if top_feats:
    n=len(top_feats); r=int(np.ceil(n/3))
    plt.figure(figsize=(15, 4*r))
    for i,c in enumerate(top_feats,1):
        plt.subplot(r,3,i); sns.boxplot(x=df[c], orient="h"); plt.title(c)
    plt.tight_layout(); plt.show()

if len(num_cols)>=2:
    corr = df[num_cols].corr(method="pearson")
    pairs=[]
    cols=corr.columns.tolist()
    for i in range(len(cols)):
        for j in range(i+1,len(cols)):
            pairs.append((cols[i], cols[j], corr.iloc[i,j], abs(corr.iloc[i,j])))
    corr_pairs=pd.DataFrame(pairs, columns=["feat_a","feat_b","r","abs_r"]).sort_values("abs_r", ascending=False)
    display(corr_pairs.head(30))

import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns
sns.set_context("notebook")

num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
corr = df[num_cols].corr(method="pearson")

mask = np.triu(np.ones_like(corr, dtype=bool))
plt.figure(figsize=(max(8, 0.5*len(num_cols)), max(6, 0.5*len(num_cols))))
sns.heatmap(corr, mask=mask, cmap="coolwarm", vmin=-1, vmax=1, center=0, square=True, linewidths=.3, cbar_kws={"shrink": .8})
plt.title("Correlation heatmap")
plt.show()

vif_df = compute_vif(df[[c for c in num_cols if df[c].notna().sum()>0]][:]) if len(num_cols)>=2 else pd.DataFrame(columns=["feature","vif","aux_r2"])
display(vif_df.head(20))

"""FEATURE ENGINEERING"""

df=pd.read_csv(PATH, parse_dates=[c for c in ["score_date"] if c in pd.read_csv(PATH, nrows=0).columns])
df.columns=df.columns.str.strip().str.lower().str.replace(r"\s+","_",regex=True)
df.head()

w=df.copy()
cols_nonneg=["revolving_balance_sum","revolving_limit_sum","total_balance_sum","collection_balance_total",
             "inquiries_12mo_hard","inquiries_12mo_soft","delinq_24mo_count",
             "tradelines_open","tradelines_total","months_since_oldest_account","avg_account_age_months"]
for c in cols_nonneg:
    if c in w: w[c]=w[c].clip(lower=0)
if "revolving_utilization" in w: w["revolving_utilization"]=w["revolving_utilization"].clip(0,1)
w.head()

if "revolving_utilization" in w:
    w["utilization"]=w["revolving_utilization"]
elif {"revolving_balance_sum","revolving_limit_sum"}<=set(w.columns):
    w["utilization"]=(w["revolving_balance_sum"]/w["revolving_limit_sum"].replace(0,np.nan)).clip(0,1)

if "avg_account_age_months" in w: w["age_years"]=(w["avg_account_age_months"]/12.0).clip(0,30)
if "inquiries_12mo_hard" in w: w["inquiries"]=w["inquiries_12mo_hard"].clip(0,6)
if "tradelines_open" in w: w["credit_mix_count"]=w["tradelines_open"].clip(0,12)

kpi_cols=[c for c in ["utilization","age_years","inquiries","credit_mix_count"] if c in w.columns]
w[kpi_cols].describe().T.round()

if "collection_balance_total" in w:
    w["collection_positive"]=(w["collection_balance_total"]>0).astype(float)
    w["collection_balance_log1p"]=np.log1p(w["collection_balance_total"])

if "delinq_24mo_count" in w:
    d=w["delinq_24mo_count"].clip(0,10)
    w["delinq_bin0"]=(d==0).astype(float)
    w["delinq_bin1_2"]=((d>=1)&(d<=2)).astype(float)
    w["delinq_bin3_4"]=((d>=3)&(d<=4)).astype(float)
    w["delinq_bin5p"]=(d>=5).astype(float)

if "has_bankruptcy" in w: w["has_bankruptcy"]=w["has_bankruptcy"].astype(float)
if "has_collection" in w: w["has_collection"]=w["has_collection"].astype(float)

ph_cols=[c for c in ["collection_positive","collection_balance_log1p","has_bankruptcy",
                     "delinq_bin0","delinq_bin1_2","delinq_bin3_4","delinq_bin5p","has_collection"] if c in w.columns]
w[ph_cols].head()

if "collection_balance_log1p" in w:
    q99=max(w["collection_balance_log1p"].quantile(0.99),1.0)
    w["collection_balance_log1p"]=w["collection_balance_log1p"].clip(0,q99)
w.head()

import numpy as np
import pandas as pd

def zclip01(s, lo=0.02, hi=0.98):
    s = pd.to_numeric(s, errors='coerce').astype(float)
    ql, qh = np.nanquantile(s, lo), np.nanquantile(s, hi)
    return np.clip((s - ql) / (qh - ql + 1e-12), 0, 1)

def build_components_v1(X: pd.DataFrame) -> pd.DataFrame:
    X = X.copy()
    delinq = X.get('delinq_24mo_count', pd.Series(0, index=X.index)).fillna(0)
    worst  = X.get('worst_delinq_24mo', pd.Series(0, index=X.index)).fillna(0)
    has_col = X.get('has_collection', pd.Series(0, index=X.index)).fillna(0).astype(float)
    col_bal = X.get('collection_balance_total', pd.Series(0, index=X.index)).fillna(0)
    has_bk  = X.get('has_bankruptcy', pd.Series(0, index=X.index)).fillna(0).astype(float)

    ph = 1.0 - 0.50*zclip01(delinq) - 0.35*zclip01(worst) - 0.25*has_col - 0.12*zclip01(col_bal) - 0.75*has_bk
    payment_history = np.clip(ph, 0, 1)

    util = X.get('revolving_utilization')
    if util is None or util.isna().all():
        bal = X.get('revolving_balance_sum', pd.Series(np.nan, index=X.index))
        lim = X.get('revolving_limit_sum', pd.Series(np.nan, index=X.index)).replace(0, np.nan)
        util = (bal / lim)
    util = util.clip(0, 1).fillna(util.median())
    total_bal = X.get('total_balance_sum', pd.Series(0, index=X.index)).fillna(0)

    amounts_owed = np.clip(1.0 - 0.8*util - 0.2*zclip01(total_bal), 0, 1)

    age_months = X.get('avg_account_age_months', X.get('months_since_oldest_account', pd.Series(0, index=X.index)))
    age_years = (age_months/12.0).fillna(0)
    length_history = zclip01(age_years.clip(0, 35))

    inq = X.get('inquiries_12mo_hard', pd.Series(0, index=X.index)).fillna(0)
    new_credit = 1.0 - zclip01(inq)

    tl_open = X.get('tradelines_open', pd.Series(0, index=X.index)).fillna(0)
    tl_total = X.get('tradelines_total', pd.Series(0, index=X.index)).fillna(tl_open)
    tl_ratio = (tl_open / (tl_total.replace(0, np.nan))).fillna(0)
    mix01 = 0.6*zclip01(tl_open) + 0.4*zclip01(tl_ratio)
    credit_mix = 0.5*mix01 + 0.5*(1 - np.abs(mix01 - 0.6))

    return pd.DataFrame({
        'payment_history': payment_history,
        'amounts_owed': amounts_owed,
        'length_history': length_history,
        'new_credit': new_credit,
        'credit_mix': credit_mix
    }, index=X.index)

def piecewise_util(u):
    u = u.clip(0, 1).fillna(u.median())
    a = (u <= 0.10).astype(float)
    b = ((u > 0.10) & (u <= 0.30)).astype(float)
    c = ((u > 0.30) & (u <= 0.50)).astype(float)
    d = (u > 0.50).astype(float)
    return pd.DataFrame({'util_a_<=10': a, 'util_b_10_30': 0.7*b, 'util_c_30_50': 0.3*c, 'util_d_>50': 0.0*d}, index=u.index)

def piecewise_inq(q):
    q = q.fillna(q.median())
    a = (q == 0).astype(float)
    b = (q == 1).astype(float)
    c = (q == 2).astype(float)
    d = (q >= 3).astype(float)
    return pd.DataFrame({'inq0': a, 'inq1': 0.7*b, 'inq2': 0.4*c, 'inq3p': 0.0*d}, index=q.index)

def piecewise_age_years(ay):
    ay = ay.clip(lower=0).fillna(0)
    a = (ay >= 15).astype(float)
    b = ((ay >= 7) & (ay < 15)).astype(float)
    c = ((ay >= 3) & (ay < 7)).astype(float)
    d = (ay < 3).astype(float)
    return pd.DataFrame({'age15p': a, 'age7_15': 0.7*b, 'age3_7': 0.4*c, 'age_lt3': 0.0*d}, index=ay.index)

def build_components_v2(X: pd.DataFrame) -> pd.DataFrame:
    X = X.copy()
    delinq = X.get('delinq_24mo_count', pd.Series(0, index=X.index)).fillna(0)
    worst  = X.get('worst_delinq_24mo', pd.Series(0, index=X.index)).fillna(0)
    has_col = X.get('has_collection', pd.Series(0, index=X.index)).fillna(0).astype(float)
    col_bal = X.get('collection_balance_total', pd.Series(0, index=X.index)).fillna(0)
    has_bk  = X.get('has_bankruptcy', pd.Series(0, index=X.index)).fillna(0).astype(float)

    ph_core = 1.0 - 0.50*zclip01(delinq) - 0.35*zclip01(worst) - 0.25*has_col - 0.12*zclip01(col_bal) - 0.75*has_bk
    payment_history = np.clip(ph_core, 0, 1)

    util = X.get('revolving_utilization')
    if util is None or util.isna().all():
        bal = X.get('revolving_balance_sum', pd.Series(np.nan, index=X.index))
        lim = X.get('revolving_limit_sum', pd.Series(np.nan, index=X.index)).replace(0, np.nan)
        util = (bal / lim)
    util = util.clip(0, 1).fillna(util.median())
    util_pw = piecewise_util(util)

    total_bal = X.get('total_balance_sum', pd.Series(0, index=X.index)).fillna(0)
    owed_soft = np.clip(1.0 - 0.2*zclip01(total_bal), 0, 1)

    age_months = X.get('avg_account_age_months', X.get('months_since_oldest_account', pd.Series(0, index=X.index)))
    age_years = (age_months/12.0).fillna(0)
    age_pw = piecewise_age_years(age_years)

    inq = X.get('inquiries_12mo_hard', pd.Series(0, index=X.index)).fillna(0)
    inq_pw = piecewise_inq(inq)

    tl_open = X.get('tradelines_open', pd.Series(0, index=X.index)).fillna(0)
    tl_total = X.get('tradelines_total', pd.Series(0, index=X.index)).fillna(tl_open)
    tl_ratio = (tl_open / (tl_total.replace(0, np.nan))).fillna(0)
    mix01 = 0.6*zclip01(tl_open) + 0.4*zclip01(tl_ratio)

    comp = pd.DataFrame({
        'payment_history': payment_history,
        'owed_soft': owed_soft,
        'mix_core': 0.5*mix01 + 0.5*(1 - np.abs(mix01 - 0.6))
    }, index=X.index)
    comp = pd.concat([comp, util_pw, age_pw, inq_pw], axis=1).fillna(0.0)
    return comp

BUILD = "v2"
build_components = build_components_v2 if BUILD == "v2" else build_components_v1

from sklearn.model_selection import GroupKFold
from sklearn.isotonic import IsotonicRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from scipy.optimize import nnls
import numpy as np

def learn_weights_nnls(S: pd.DataFrame, y: pd.Series):
    X = S.to_numpy(float)
    yv = pd.to_numeric(y, errors='coerce').astype(float)
    yv = (yv - np.nanmean(yv)) / (np.nanstd(yv) + 1e-9)
    w, _ = nnls(X, yv)
    w = w / (w.sum() + 1e-12)
    return w

def raw_score(S: pd.DataFrame, w, mm=None):
    r = S.to_numpy(float) @ np.asarray(w, float)
    if mm is None:
        rmin, rmax = r.min(), r.max()
    else:
        rmin, rmax = mm
    r01 = (r - rmin) / (rmax - rmin + 1e-12)
    return pd.Series(r01, index=S.index), (rmin, rmax)

def group_kfold_eval(df: pd.DataFrame, ycol: str = "fico8", groupcol: str = "consumer_id", k: int = 5):
    S = build_components(df)
    y = df[ycol].astype(float)
    groups = df[groupcol]
    gkf = GroupKFold(n_splits=k)
    maes, rmses, r2s = [], [], []
    for tr, te in gkf.split(S, y, groups):
        S_tr, S_te = S.iloc[tr], S.iloc[te]
        ytr, yte = y.iloc[tr], y.iloc[te]
        w = learn_weights_nnls(S_tr, ytr)
        r_tr, mm = raw_score(S_tr, w)
        r_te, _  = raw_score(S_te, w, mm=mm)
        iso = IsotonicRegression(y_min=300, y_max=850, increasing=True, out_of_bounds='clip')
        iso.fit(r_tr.values, ytr.values)
        yhat = pd.Series(iso.predict(r_te.values), index=S_te.index)
        mae  = mean_absolute_error(yte, yhat)
        rmse = np.sqrt(mean_squared_error(yte, yhat))
        r2   = r2_score(yte, yhat)
        maes.append(mae); rmses.append(rmse); r2s.append(r2)
    print(f"[FICO8 | GroupKFold={k}] MAE {np.mean(maes):.1f}±{np.std(maes):.1f} | RMSE {np.mean(rmses):.1f}±{np.std(rmses):.1f} | R² {np.mean(r2s):.3f}±{np.std(r2s):.3f}")

group_kfold_eval(df, ycol="fico8", groupcol="consumer_id", k=5)

df[['consumer_id','fico8','vantage4','credit_score_pred']].head(10)

import numpy as np
import pandas as pd
from sklearn.model_selection import GroupShuffleSplit
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.isotonic import IsotonicRegression
from scipy.optimize import nnls

try:
    df
except NameError:
    df = pd.read_csv("cleaned_feature_view.csv")

for col in ["consumer_id", "fico8", "score_date"]:
    if col not in df.columns:
        df[col] = pd.NaT if col == "score_date" else np.nan

df["score_date"] = pd.to_datetime(df.get("score_date", pd.NaT), errors="coerce")

def zclip01(s, lo=0.02, hi=0.98):
    s = pd.to_numeric(s, errors="coerce").astype(float)
    ql, qh = np.nanquantile(s, lo), np.nanquantile(s, hi)
    return np.clip((s - ql) / (qh - ql + 1e-12), 0, 1)

def piecewise_util(u):
    u = u.clip(0, 1).fillna(u.median())
    a = (u <= 0.10).astype(float)
    b = ((u > 0.10) & (u <= 0.30)).astype(float)
    c = ((u > 0.30) & (u <= 0.50)).astype(float)
    d = (u > 0.50).astype(float)
    return pd.DataFrame({"util_a_<=10": a, "util_b_10_30": 0.7*b, "util_c_30_50": 0.3*c, "util_d_>50": 0.0*d}, index=u.index)

def piecewise_inq(q):
    q = q.fillna(q.median())
    a = (q == 0).astype(float)
    b = (q == 1).astype(float)
    c = (q == 2).astype(float)
    d = (q >= 3).astype(float)
    return pd.DataFrame({"inq0": a, "inq1": 0.7*b, "inq2": 0.4*c, "inq3p": 0.0*d}, index=q.index)

def piecewise_age_years(ay):
    ay = ay.clip(lower=0).fillna(0)
    a = (ay >= 15).astype(float)
    b = ((ay >= 7) & (ay < 15)).astype(float)
    c = ((ay >= 3) & (ay < 7)).astype(float)
    d = (ay < 3).astype(float)
    return pd.DataFrame({"age15p": a, "age7_15": 0.7*b, "age3_7": 0.4*c, "age_lt3": 0.0*d}, index=ay.index)

def build_components(X: pd.DataFrame) -> pd.DataFrame:
    X = X.copy()
    delinq = X.get("delinq_24mo_count", pd.Series(0, index=X.index)).fillna(0)
    worst  = X.get("worst_delinq_24mo", pd.Series(0, index=X.index)).fillna(0)
    has_col = X.get("has_collection", pd.Series(0, index=X.index)).fillna(0).astype(float)
    col_bal = X.get("collection_balance_total", pd.Series(0, index=X.index)).fillna(0)
    has_bk  = X.get("has_bankruptcy", pd.Series(0, index=X.index)).fillna(0).astype(float)
    ph_core = 1.0 - 0.50*zclip01(delinq) - 0.35*zclip01(worst) - 0.25*has_col - 0.12*zclip01(col_bal) - 0.75*has_bk
    payment_history = np.clip(ph_core, 0, 1)
    util = X.get("revolving_utilization")
    if util is None or util.isna().all():
        bal = X.get("revolving_balance_sum", pd.Series(np.nan, index=X.index))
        lim = X.get("revolving_limit_sum", pd.Series(np.nan, index=X.index)).replace(0, np.nan)
        util = (bal / lim)
    util = util.clip(0, 1).fillna(util.median())
    util_pw = piecewise_util(util)
    total_bal = X.get("total_balance_sum", pd.Series(0, index=X.index)).fillna(0)
    owed_soft = np.clip(1.0 - 0.2*zclip01(total_bal), 0, 1)
    age_months = X.get("avg_account_age_months", X.get("months_since_oldest_account", pd.Series(0, index=X.index)))
    age_years = (age_months/12.0).fillna(0)
    age_pw = piecewise_age_years(age_years)
    inq = X.get("inquiries_12mo_hard", pd.Series(0, index=X.index)).fillna(0)
    inq_pw = piecewise_inq(inq)
    tl_open = X.get("tradelines_open", pd.Series(0, index=X.index)).fillna(0)
    tl_total = X.get("tradelines_total", pd.Series(0, index=X.index)).fillna(tl_open)
    tl_ratio = (tl_open / (tl_total.replace(0, np.nan))).fillna(0)
    mix01 = 0.6*zclip01(tl_open) + 0.4*zclip01(tl_ratio)
    comp = pd.DataFrame({
        "payment_history": payment_history,
        "owed_soft": owed_soft,
        "mix_core": 0.5*mix01 + 0.5*(1 - np.abs(mix01 - 0.6))
    }, index=X.index)
    comp = pd.concat([comp, util_pw, age_pw, inq_pw], axis=1).fillna(0.0)
    return comp

def learn_weights_nnls(S: pd.DataFrame, y: pd.Series):
    X = S.to_numpy(float)
    yv = pd.to_numeric(y, errors="coerce").astype(float)
    yv = (yv - np.nanmean(yv)) / (np.nanstd(yv) + 1e-9)
    w, _ = nnls(X, yv)
    w = w / (w.sum() + 1e-12)
    return w

def raw_score(S: pd.DataFrame, w, mm=None):
    r = S.to_numpy(float) @ np.asarray(w, float)
    if mm is None:
        rmin, rmax = r.min(), r.max()
    else:
        rmin, rmax = mm
    r01 = (r - rmin) / (rmax - rmin + 1e-12)
    return pd.Series(r01, index=S.index), (rmin, rmax)

def make_train_test_masks(df, ycol="fico8", groupcol="consumer_id", timecol="score_date",
                          test_size=0.2, random_state=42):
    valid = df[ycol].notna()
    if valid.sum() < 2:
        raise ValueError(f"Need at least 2 rows with non-null {ycol}; found {valid.sum()}.")
    tr_mask = pd.Series(False, index=df.index)
    te_mask = pd.Series(False, index=df.index)
    if timecol in df.columns and df[timecol].notna().any():
        dates = pd.to_datetime(df[timecol], errors="coerce")
        cut = dates[valid].quantile(1 - test_size)
        tr_mask = valid & (dates <= cut)
        te_mask = valid & (dates > cut)
    if tr_mask.sum() == 0 or te_mask.sum() == 0:
        gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)
        groups = df.loc[valid, groupcol] if groupcol in df.columns else np.arange(valid.sum())
        tr_idx, te_idx = next(gss.split(df.loc[valid], groups=groups))
        tr_mask = pd.Series(False, index=df.index); te_mask = pd.Series(False, index=df.index)
        tr_mask.loc[df.index[valid][tr_idx]] = True
        te_mask.loc[df.index[valid][te_idx]] = True
    if tr_mask.sum() == 0 or te_mask.sum() == 0:
        idx = df.index[valid].to_numpy()
        rng = np.random.RandomState(random_state); rng.shuffle(idx)
        split = max(1, int(len(idx) * (1 - test_size)))
        tr_mask = pd.Series(False, index=df.index); tr_mask.loc[idx[:split]] = True
        te_mask = pd.Series(False, index=df.index); te_mask.loc[idx[split:]] = True
    if tr_mask.sum() == 0 or te_mask.sum() == 0:
        raise ValueError("Could not create non-empty train/test splits.")
    return tr_mask, te_mask

tr_mask, te_mask = make_train_test_masks(df, ycol="fico8", groupcol="consumer_id", timecol="score_date", test_size=0.2)
S_all = build_components(df)
y_all = df["fico8"].astype(float)
S_tr, S_te = S_all[tr_mask], S_all[te_mask]
ytr, yte = y_all[tr_mask], y_all[te_mask]

w = learn_weights_nnls(S_tr, ytr)
r_tr, mm = raw_score(S_tr, w)
r_te, _  = raw_score(S_te, w, mm=mm)

iso = IsotonicRegression(y_min=300, y_max=850, increasing=True, out_of_bounds="clip")
iso.fit(r_tr.values, ytr.values)
yhat_te = pd.Series(iso.predict(r_te.values), index=S_te.index)

mae  = mean_absolute_error(yte, yhat_te)
rmse = np.sqrt(mean_squared_error(yte, yhat_te))
r2   = r2_score(yte, yhat_te)
print(f"[FICO8 Holdout | global iso] MAE {mae:.1f} | RMSE {rmse:.1f} | R² {r2:.3f}")

def quarter_key(s):
    s = pd.to_datetime(s, errors="coerce")
    return s.dt.to_period("Q").astype(str)

use_per_quarter = df["score_date"].notna().any()
if use_per_quarter:
    key = quarter_key(df["score_date"])
    yhat_te_q = pd.Series(index=S_te.index, dtype=float)
    for q in key[tr_mask].unique():
        tr_idx = tr_mask & (key == q)
        te_idx = te_mask & (key == q)
        if te_idx.sum() == 0 or tr_idx.sum() < 15:
            continue
        r_tr_q, _ = raw_score(S_all[tr_idx], w, mm=mm)
        r_te_q, _ = raw_score(S_all[te_idx], w, mm=mm)
        iso_q = IsotonicRegression(y_min=300, y_max=850, increasing=True, out_of_bounds="clip")
        iso_q.fit(r_tr_q.values, y_all[tr_idx].values)
        yhat_te_q.loc[te_idx[te_idx].index] = iso_q.predict(r_te_q.values)
    mask_eval = yhat_te_q.notna()
    if mask_eval.any():
        mae_q  = mean_absolute_error(yte[mask_eval], yhat_te_q[mask_eval])
        rmse_q = np.sqrt(mean_squared_error(yte[mask_eval], yhat_te_q[mask_eval]))
        r2_q   = r2_score(yte[mask_eval], yhat_te_q[mask_eval])
        print(f"[FICO8 Holdout | per-quarter iso] MAE {mae_q:.1f} | RMSE {rmse_q:.1f} | R² {r2_q:.3f}")

r_all, _ = raw_score(S_all, w, mm=mm)
if use_per_quarter:
    key_all = quarter_key(df["score_date"]) if df["score_date"].notna().any() else pd.Series("ALL", index=df.index)
    yhat_all = pd.Series(index=df.index, dtype=float)
    for q in key_all.unique():
        idx = (key_all == q)
        r_q = r_all[idx]
        tr_idx = tr_mask & (key_all == q)
        if r_q.empty or tr_idx.sum() < 15:
            yhat_all.loc[idx] = iso.predict(r_q.values)
            continue
        r_tr_q, _ = raw_score(S_all[tr_idx], w, mm=mm)
        iso_q = IsotonicRegression(y_min=300, y_max=850, increasing=True, out_of_bounds="clip")
        iso_q.fit(r_tr_q.values, y_all[tr_idx].values)
        yhat_all.loc[idx] = iso_q.predict(r_q.values)
    df["credit_score_pred"] = yhat_all.clip(300, 850)
else:
    df["credit_score_pred"] = pd.Series(iso.predict(r_all.values), index=r_all.index).clip(300, 850)

def predict_scores(new_df: pd.DataFrame):
    Snew_all = build_components(new_df)
    r = Snew_all[S_all.columns].to_numpy(float) @ w
    r01 = (r - mm[0]) / (mm[1] - mm[0] + 1e-12)
    yhat = iso.predict(r01)
    out = new_df.copy()
    out["credit_score_pred"] = np.clip(yhat, 300, 850)
    return out

def predict_one(sample_dict: dict):
    x = pd.DataFrame([sample_dict])
    out = predict_scores(x)
    comps = build_components(x)[S_all.columns]
    contrib = (comps.iloc[0].values * w)
    return float(out.loc[0, "credit_score_pred"]), dict(zip(S_all.columns, contrib))

sample = {
    "revolving_utilization": 0.22,
    "revolving_balance_sum": 3500,
    "revolving_limit_sum": 16000,
    "total_balance_sum": 12000,
    "delinq_24mo_count": 1,
    "worst_delinq_24mo": 2,
    "has_collection": 0,
    "collection_balance_total": 0,
    "has_bankruptcy": 0,
    "avg_account_age_months": 84,
    "inquiries_12mo_hard": 1,
    "tradelines_open": 6,
    "tradelines_total": 8
}

score, contrib = predict_one(sample)
print("Predicted score (example):", round(score))

import numpy as np, pandas as pd

ys = df['fico8'].astype(float).dropna()
naive_mae = np.mean(np.abs(ys - ys.mean()))
naive_rmse = np.sqrt(np.mean((ys - ys.mean())**2))
print("Naive-mean baseline MAE/RMSE:", round(naive_mae,1), round(naive_rmse,1))

if 'vantage4' in df.columns:
    m = df[['fico8','vantage4']].dropna()
    corr = m.corr().iloc[0,1]
    mae_cross = np.mean(np.abs(m['fico8'] - m['vantage4']))
    print("FICO8 vs Vantage4 corr/MAE:", round(corr,3), round(mae_cross,1))

